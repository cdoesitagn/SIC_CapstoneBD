{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "from gensim.models import FastText\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Dịch ngược với Deep Translator\n",
    "\"\"\"def back_translation_deep_translator(doc):\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    back_translator = GoogleTranslator(source='en', target='vi')\n",
    "\n",
    "    def translate_text(text, max_retries=10, delay=1):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                translated = translator.translate(text)\n",
    "                back_translated = back_translator.translate(translated)\n",
    "                return back_translated\n",
    "            except Exception as e:\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        print('Không thể dịch')\n",
    "        return text\n",
    "\n",
    "    chunk_size = 1000\n",
    "    if len(doc) > chunk_size:\n",
    "        chunks = [doc[i:i + chunk_size] for i in range(0, len(doc), chunk_size)]\n",
    "        results = [translate_text(chunk) for chunk in chunks]\n",
    "        return ' '.join(results)\n",
    "    else:\n",
    "        return translate_text(doc)\"\"\"\n",
    "\n",
    "\n",
    "# Thay thế từ đồng nghĩa\n",
    "def synonym_replacement(doc, synonym_dict):\n",
    "    tokens = word_tokenize(doc)\n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word.lower() in synonym_dict:\n",
    "            if synonym_dict[word.lower()]:\n",
    "                new_tokens.append(random.choice(list(synonym_dict[word.lower()])))\n",
    "            else:\n",
    "                new_tokens.append(word.lower())\n",
    "        else:\n",
    "            new_tokens.append(word)\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "\n",
    "# Tăng cường dữ liệu\n",
    "def augmentation_text(data, option,  synonym_dict, num_new_texts, max_workers = 10):\n",
    "    augmented_docs = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    for label in data[option].unique():\n",
    "        label_data = data[data[option] == label]\n",
    "        \n",
    "        def process_document(idx):\n",
    "            doc = data.loc[idx, 'Text']\n",
    "            new_doc = synonym_replacement(doc, synonym_dict)\n",
    "            #new_doc = back_translation_deep_translator(new_doc)\n",
    "            return new_doc\n",
    "        \n",
    "        indices = [random.choice(label_data.index) for _ in range(num_new_texts)]\n",
    "        labels.extend(data.loc[indices, option])\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                results = list(executor.map(process_document, indices)\n",
    "                               ) \n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi xảy ra trong ThreadPoolExecutor: {e}\")\n",
    "        augmented_docs.extend(results)\n",
    "    \n",
    "    return augmented_docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo thư viện từ đồng nghĩa\n",
    "def create_synonyms_dict(directory):\n",
    "    synonyms_dict = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        line = line.strip()\n",
    "                        if line:  \n",
    "                            synonyms = line.split(',') \n",
    "\n",
    "                            if len(synonyms) == 1:\n",
    "                                continue\n",
    "\n",
    "                            for word in synonyms:\n",
    "                                word = word.strip()\n",
    "                                if word:\n",
    "                                    if word not in synonyms_dict:\n",
    "                                        synonyms_dict[word] = set()\n",
    "                                    synonyms_dict[word].update(synonym.strip() for synonym in synonyms if synonym.strip() != word)\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi {e}\")\n",
    "\n",
    "    return synonyms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#translator = GoogleTranslator()\n",
    "directory = '..\\\\..\\\\vi-wordnet'\n",
    "synonyms_dict = create_synonyms_dict(directory)\n",
    "num_news_train = 1000\n",
    "num_news_test = 300\n",
    "max_worker = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('..\\\\..\\\\data\\\\processed\\\\labeled_data.csv')\n",
    "\n",
    "for option in ['Near', 'Mid', 'Far', 'Potential']:\n",
    "    df = data[['Text', option]]\n",
    "    documents = df\n",
    "    label = data.loc[:, option]\n",
    "    doc_train, doc_test, label_train, label_test = train_test_split(documents, label, test_size=0.3, random_state=42)\n",
    "    \n",
    "    new_acticles_train = augmentation_text(doc_train, option, synonyms_dict, num_new_texts=num_news_train, max_workers=max_worker)\n",
    "    new_acticles_test = augmentation_text(doc_test, option, synonyms_dict, num_new_texts=num_news_test, max_workers=max_worker)\n",
    "\n",
    "    new_acticles_train = pd.DataFrame({'Text' : new_acticles_train[0], option : new_acticles_train[1]})\n",
    "    new_acticles_train['origin'] = 1\n",
    "    new_acticles_test = pd.DataFrame({'Text' : new_acticles_test[0], option : new_acticles_test[1]})\n",
    "    new_acticles_test['origin'] = 1\n",
    "\n",
    "    doc_train = pd.DataFrame(doc_train)\n",
    "    doc_train['origin'] = 0\n",
    "    doc_test = pd.DataFrame(doc_test)\n",
    "    doc_test['origin'] = 0\n",
    "    doc_test_gr = pd.concat([doc_test, new_acticles_test], axis = 0)\n",
    "    doc_train_gr = pd.concat([doc_train, new_acticles_train], axis = 0)\n",
    "\n",
    "    doc_train_gr.to_csv(f'..\\\\..\\\\data\\\\processed\\\\{option}_train.csv')\n",
    "    doc_test_gr.to_csv(f'..\\\\..\\\\data\\\\processed\\\\{option}_test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
